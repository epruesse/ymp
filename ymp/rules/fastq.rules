from ymp.config import icfg
import ymp
import os


"""
This file contains the rules dealing with FQ file provisioning and preparation. 
"""

###
### Meta rules
###

localrules: fq_all
rule fq_all:
    message: "Finished {wildcards.dir}"
    input: "{dir}/{:fq_names:}.fq.gz"
    output: touch("{dir}/all")

rule fastqc_all:
    message: "Finished {wildcards.dir}"
    input: "{dir}.fastqc/{:fq_names:}_fastqc.zip"
    output: touch("{dir}.fastqc/all")



###
###  SRA access 
###


localrules: prefetch
rule prefetch:
    """
    Downloads SRA files into NCBI SRA folder (ncbi/public/sra).
    """
    message:
        "Pre-Fetching {wildcards.SRR}"
    output:
        "{:dir.sra:}/{SRR}.sra"
    conda:
        "sratools.yml"
    shell: """
    prefetch {wildcards.SRR}
    """


rule fastq_dump:
    """
    Extracts FQ from SRA files
    """
    message:
        "Extracting FastQ from {wildcards.SRR}"
    output:
        "{:dir.scratch:}/SRR/{SRR}_1.fastq.gz",
        "{:dir.scratch:}/SRR/{SRR}_2.fastq.gz"
    params:
        outdir = icfg.dir.scratch + "/SRR",
        p = lambda wc,threads: int(threads/2+.5)
    conda:
        "sratools.yml"
    threads:
        4
    # FIXME
    # the two cut processes use about 1 cpu each, fastqdump 1/4 and pgzip about 1 each.
    # not ideal. not sure why cut needs so much time. 
    shell: """
    fastq-dump {wildcards.SRR} \
        --split-files \
        --readids \
        --dumpbase \
        --skip-technical \
        --clip \
        --read-filter pass \
        --stdout | \
      paste - - - -  - - - - | \
      tee >(cut -f 1-4 | tr "\t" "\\n" | pigz -p {params.p} > {output[0]}) | \
      cut -f 5-8 | tr "\t" "\\n" | pigz -p {params.p} > {output[1]}
    """


rule export_qiime_map_file:
    message:
        "Creating Qiime map file for project {wildcards.project}"
    output:
        "{project}/qiime_mapping.tsv"
    run:
        import pandas as pd
        df = icfg[wildcards.project].run_data
        cols = df.columns.tolist()

        try:
            desc_idx = cols.index("Description")
            cols = cols[:desc_idx] + cols[desc_idx+1:] + [cols[desc_idx]]
            df = df[cols]
        except ValueError:
            df["Description"] = ""

        df.to_csv(output[0], sep="\t", index=False)

        # TODO: Rename bccol to "BarcodeSequence"
        #       Fake LinkerPrimerSequence col if not exists
        #       Make first/index column be called "#SampleID"


rule split_library:
    """
    Splits barcode indexed files into separate fq files
    """
    message:
        "Splitting library (barcodes: {input[0]}, reads: {input[1]})"
    input:
        lambda wc: icfg[wc.project].unsplit_path(wc.barcodes, wc.pairname),
        mapping = "{project}/qiime_mapping.tsv",
        tmpdir  = ancient(icfg.dir.tmp)
    output:
        outdir  = temp("{project}.split_libraries/{barcodes}/{pairname}/")
    log:
        "{project}.split_libraries/{barcodes}/{pairname}/split_library_log.txt",
        "{project}.split_libraries/{barcodes}/{pairname}/split_seq_file_log.txt",
    conda:
        "qiime.yml"
    shell: """
    split_libraries_fastq.py \
       -b {input[0]} \
       -i {input[1]} \
       -m {input.mapping} \
       --store_demultiplexed_fastq \
       --max_bad_run_length=1000000 \
       --min_per_read_length_fraction=0.000001 \
       --sequence_max_n=100000 \
       --phred_quality_threshold=1 \
       -o {output}

    split_sequence_file_on_sample_ids.py \
       -i {output}/seqs.fastq \
       --file_type fastq \
       -o {output} \
       > {log[1]} 2>&1
    """

rule split_library_compress_sample:
    message:
        "Compressing {wildcards.sample}.{wildcards.pairname}.fq.gz"
    input:
        "{project}.split_libraries/{barcodes}/{pairname}/"
    output:
        "{project}.split_libraries/{barcodes}/{sample}.{pairname}.fq.gz"
    conda:
        "pigz.yml"
    shell: """
    pigz -6 -c <{input}/{wildcards.sample}.fastq >{output}
    """

###
###  Linking into current workspace
###
    

localrules: symlink_raw_reads
rule symlink_raw_reads:
    """Normalize FQ names by creating symlinks to original files"""
    message:
        "Creating symlink {output} -> {input}"
    input:
        # extract path from config file:
        lambda wc: icfg[wc.project].FQpath(wc.run, wc.pairsuff)
    output:
        "{project}/{run}.{pairsuff}.fq.gz"
    run:
        if not os.path.isabs(input[0]):
            input[0] = os.path.join("..", input[0])
        os.symlink(input[0], output[0])



###
###  Quality Reports
###


rule fastqc:
    """Run FastQC on read files"""
    message: "Creating QC report for {input}"
    input:   "{dir}/{file}.fq.gz"
    output:  "{dir}.fastqc/{file}_fastqc.html",
             "{dir}.fastqc/{file}_fastqc.zip"
    log:     "{dir}.fastqc/{file}_fastqc.log"
    threads: 1
    params:  k=10,
             mem=icfg.mem("8g")
    conda:   "fastqc.yml"
    shell: """
    # override too low memory requested by fastqc 0.11.5 of 256M per thread:
    export _JAVA_OPTIONS="-Xmx{params.mem}m"
    fastqc -t {threads} -o $(dirname {output[0]}) {input} -k 10 >{log} 2>&1
    """


rule multiqc:
    """Assemble report on all FQ files in a directory"""
    message: "Aggregating QC reports for {wildcards.dir}"
    input:   "{dir}.fastqc/{:fq_names:}_fastqc.zip"
    output:  "{:dir.reports:}/{dir}.fastqc.html",
    log:     "{dir}.fastqc/multiqc.log"
    threads: 1
    params:  dir = "{dir}.fastqc"
    conda:   "multiqc.yml"
    shell: """
    multiqc \
            --module fastqc \
            --outdir {params.dir} \
            --title  {wildcards.dir} \
            --force \
            {wildcards.dir}.fastqc \
            > {log} 2>&1
    if [ -e  {params.dir}/multiqc_report.html ]; then
        mv {params.dir}/multiqc_report.html {output[0]}
    else
        # never versions of multiqc put title in output file name
        mv {params.dir}/{wildcards.dir}_multiqc_report.html {output[0]}
    fi
    """


rule phyloFlash_makedb:
    """Have phyloFlash build its reference database"""
    message: "PhyloFlash running makedb.pl"
    output:  "{:dir.references:}/phyloFlash"
    log:     "{:dir.references:}/phyloFlash/build.log"
    input:   ssu="{:dir.references:}/ssu.fasta.gz",
             univec="{:dir.references:}/UniVec.fasta.gz"
    conda:   "phyloflash.yml"
    shell: """
    CWD="$PWD"
    mkdir -p {output}
    cd {output}
    ln -s "$CWD/{input.ssu}" SILVA_0_SSU.fasta.gz
    phyloFlash_makedb.pl \
      --silva_file SILVA_0_SSU.fasta.gz \
      --univec_file "$CWD/{input.univec} >{log} 2>&1"
    """


rule phyloFlash:
    """Run PhyloFlash on samples"""
    message: "PhyloFlash {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz",
             dbhome="{:dir.references:}/phyloFlash"
    output:  "{dir}.pf/{sample}.phyloFlash.NTUabundance.csv"
    log:     "{dir}.pf/{sample}.log"
    threads: 16
    conda:   "phyloflash.yml"
    params:  in2="-read2 ../{input[1]}",
             mem=icfg.mem("80g")
    shell: """
    cd {wildcards.dir}.pf

    phyloFlash.pl -skip_emirge \
                  -skip_spades \
                  -html \
                  -readlength 301 \
                  -read1 ../{input[0]} \
                  {params.in2} \
                  -lib {wildcards.sample} \
                  -CPUs {threads} \
                  -dbhome ../{input.dbhome}/0 \
                  > ../{log} 2>&1
    """

workflow.derive_rule(
    name="phyloFlash_se",
    parent="phyloFlash",
    input="{dir}/{sample}.{:pairnames[0]:}.fq.gz",
    params={"in2":""},
    order="lesser"
)


rule phyloFlash_heatmap:
    message: "PhyloFlash Heatmap {wildcards.dir}"
    input:   "{dir}.pf/{:runs:}.phyloFlash.NTUabundance.csv"
    output:  "{:dir.reports:}/{dir}_heatmap.pdf"
    log:     "{dir}.pf/phyloFlash_heatmap.log"
    threads: 1
    conda:   "phyloflash.yml"
    shell: """
    phyloFlash_heatmap.R {input[0]} {input} --out {output} >{log} 2>&1
    """


###
###  Error correction
###


rule bb_ecco:
    """Error correction with BBMerge overlapping"""
    message: "BBTools merge ecco {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz"
    output:  "{dir}.ecco/{sample}.{:pairnames:}.fq.gz",
             adapter="{dir}.ecco/{sample}.adapter.fq"
    log:     "{dir}.ecco/{sample}.log"
    threads: 16
    params:  inout="in={input[0]} out={output[0]}",
             inout2="in2={input[1]} out2={output[1]}",
             mem=icfg.mem("80g")
    conda:   "bbmap.yml"
    shell: """
    bbmerge.sh {params.inout} {params.inout2} \
               outadapter={output.adapter} \
               ecco ecctadpole mix vstrict\
               threads={threads} -Xmx{params.mem}m \
               > {log} 2>&1
    """

workflow.derive_rule(
    name   = "bb_ecco_se",
    parent = "bb_ecco",
    input  = "{dir}/{sample}.{:pairnames[0]:}.fq.gz",
    output = "{dir}.ecco/{sample}.{:pairnames[0]:}.fq.gz",
    params = {'inout2': ""},
    order  = "lesser"
)


###
###  Trimming
###

rule trim_bbduk_adapter:
    """Trimming and Adapter Removal using BBTools BBDuk"""
    message:
        "Trimming with BBduk: {wildcards.dir}/{wildcards.sample} "
        "({params.adapt}Q={params.qual} L={params.length})"
    wildcard_constraints:
        A = "(A?)",
        Q = "(Q\d+|)",
        L = "(L\d+|)"
    input:
        "{dir}/{sample}.{:pairnames:}.fq.gz"
    output:
        "{dir}.trim{A}{Q}{L}/{sample}.{:pairnames:}.fq.gz"
    log:
        "{dir}.trim{A}{Q}{L}/{sample}.log"
    params:
        length=lambda wc: wc.L[1:] if wc.L else 20,
        qual=lambda wc: wc.Q[1:] if wc.Q else 20,
        adapt=lambda wc: 'ref=$BB_RSRC/adapters.fa ' if wc.A else '',
        k=23,
        mink=11,
        hdist=1,
        mem=icfg.mem("80g"),
        flags="pigz unpigz",
        inout="in={input[0]} out={output[0]}",
        inout2="in2={input[1]} out2={output[1]}" # overriden by child rule
    threads: 16
    conda: "bbmap.yml"
    shell:
        # find adapter dir:
        'BB_RSRC="$(dirname $(readlink -f $(command -v bbduk.sh)))/resources";'
        'bbduk.sh '
        '{params.inout} {params.inout2} '
        'trimq={params.qual} qtrim=r '         # quality trimming
        'minlength={params.length} '           # length filtering
        '{params.adapt}'                       # adapter trimming
        'ktrim=r '                             # 3' side only
        'k={params.k} '                        # k for adapter matching
        'mink={params.mink} '                  # k at read end
        'hdist={params.hdist} '                # hamming distance, allow 1 mismatch
        'tpe ' # trimpairsevenly -- in case adapter detected in only one read
        'tbo ' # trimbyoverlap -- trim if read runs over other reads' end
        '{params.flags} '                      # processing settings
        'threads={threads} '
        '-Xmx{params.mem}m '
        '>{log} 2>&1'

# Create derived child rule for single ended reads
workflow.derive_rule(
    name   = "trim_bbduk_adapter_se",
    parent = "trim_bbduk_adapter",
    input  = "{dir}/{sample}.{:pairnames[0]:}.fq.gz",
    output = "{dir}.trim{A}{Q}{L}/{sample}.{:pairnames[0]:}.fq.gz",
    params = { 'inout2': "" },
    order = "lesser"
)


rule trimmomatic_adapter:
    """Trimming with Trimmomatic"""
    message:
        "Trimmomatic {wildcards.dir}/{wildcards.sample}"
    input:
        "{dir}/{sample}.{:pairnames:}.fq.gz"
    wildcard_constraints:
        adapter="(N|T(2|3|32))"
    output:
        "{dir}.trimmomatic{adapter}/{sample}.{:pairnames[0]:}.fq.gz",
        "{dir}.trimmomatic{adapter}/{sample}.unpaired.{:pairnames[0]:}.fq.gz",
        "{dir}.trimmomatic{adapter}/{sample}.{:pairnames[1]:}.fq.gz",
        "{dir}.trimmomatic{adapter}/{sample}.unpaired.{:pairnames[1]:}.fq.gz"
    log:
        "{dir}.trimmomatic{adapter}/{sample}.log"
    params:
        seed_mismatches = 2,
        palindrome_clip_thresh = 30,
        simple_clip_thresh = 10,
        min_adapter_len = 8,
        keep_both_reads = "true",
        arg_pe="PE"
    conda:
        "trimmomatic.yml"
    threads:
        1
    shell:"""
    case {wildcards.adapter} in
      N)   ADAPTER=NexteraPE-PE.fa ;;
      T2)  ADAPTER=TruSeq2-PE.fa ;;
      T3)  ADAPTER=TruSeq3-PE.fa ;;
      T32) ADAPTER=TruSeq3-PE-2.fa ;;
    esac

    ADAPTER_DIR="$(dirname $(which trimmomatic))/../share/trimmomatic/adapters"

    CLIPARG="ILLUMINACLIP:$ADAPTER_DIR/$ADAPTER"
    CLIPARG="$CLIPARG:{params.seed_mismatches}"
    CLIPARG="$CLIPARG:{params.palindrome_clip_thresh}"
    CLIPARG="$CLIPARG:{params.simple_clip_thresh}"
    CLIPARG="$CLIPARG:{params.min_adapter_len}"
    CLIPARG="$CLIPARG:{params.keep_both_reads}"

    bash -x `which trimmomatic` {params.arg_pe} \
        -threads {threads} \
        -phred33 \
        {input} {output} \
        $CLIPARG >>{log} 2>&1
    """

workflow.derive_rule(
    name   = "trimmomatic_adapter_se",
    parent = "trimmomatic_adapter",
    input  = "{dir}/{sample}.{:pairnames[0]:}.fq.gz",
    output = "{dir}.trimmomatic{adapter}/{sample}.{:pairnames[0]:}.fq.gz",
    params = { 'arg_pe': "SE", 'outargs': "{output}" },
    order = "lesser"
)


rule sickle:
    message:
        "Trimming with Sickle {wildcards.dir}/{wildcards.sample} "
        "(Q={params.qual} L={params.length})"
    input:
        "{dir}/{sample}.{:pairnames:}.fq.gz"
    output:
        "{dir}.sickle{Q}{L}/{sample}.{:pairnames:}.fq.gz",
        "{dir}.sickle{Q}{L}/{sample}.unpaired.fq.gz",
    log:
        "{dir}.sickle{Q}{L}/{sample}.log"
    wildcard_constraints:
        Q = "(Q\d+|)",
        L = "(L\d+|)",
    params:
        length=lambda wc: wc.L[1:] if wc.L else 20,
        qual=lambda wc: wc.Q[1:] if wc.Q else 20,
        arg_pe="pe",
        inout2="-r {input[1]} -p {output[1]} -s {output[2]}"
    threads: 1
    conda:
        "sickle.yml"
    shell:"""
    sickle {params.arg_pe} \
        -f {input[0]} \
        -o {output[0]} \
        {params.inout2} \
        --qual-type=sanger \
        --length-threshold={params.length} \
        --qual-threshold={params.qual} \
        --gzip-output \
        --no-fiveprime \
        > {log} 2>&1
    """

workflow.derive_rule(
    name = "sickle_se",
    parent = "sickle",
    input = "{dir}/{sample}.{:pairnames[0]:}.fq.gz",
    output = "{dir}.sickle{Q}{L}/{sample}.{:pairnames[0]:}.fq.gz",
    params = {
        "arg_pe": "se",
        "inout2": ""
    },
    order = "lesser"
)


###
###  Contaminant filtering
###


bbfiles="scafstats refstats "
bbfiles+="covstats rpkm covhist basecov bincov"

bbstats =  "bhist qhist aqhist bqhist lhist ihist ehist qahist "
bbstats += "indelhist mhist gchist idhist statsfile"
bbstats = bbstats.split()

bbduk_stats = "bhist qhist qchist aqhist bqhist lhist gchist".split()

rule bbmap_makedb:
    message: "BBMap: preparing index for ref={input}"
    input:   "{path}/{file}.fasta.gz"
    output:  "{path}/{file}.bbidx/ref/genome/1/summary.txt",
             "{path}/{file}.bbidx"
    log:     "{path}/{file}.bbidx/bbmap_index.log"
    params:  path="{path}/{file}.bbidx/",
             mem=icfg.mem("80g")
    threads: 8
    conda:   "bbmap.yml"
    shell: """
    bbmap.sh \
        path={params.path} \
        ref={input} \
        threads={threads} \
        pigz unpigz \
        -Xmx{params.mem}m \
        >{log} 2>&1
    """


rule bbmap_split:
    message: "BBMap filtering by {wildcards.reference} - {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz",
             reference = "{:dir.references:}/{reference}.bbidx"
    output:  clean="{dir}.bbmRM{reference}/{sample}.{:pairnames:}.fq.gz",
             human="{dir}.bbmKP{reference}/{sample}.{:pairnames:}.fq.gz",
             stats=expand("{{dir}}.bbmRM{{reference}}/{{sample}}.{x}",x=bbstats)
    log:     "{dir}.bbmRM{reference}/{sample}.log"
    params:  stats=lambda wc: expand("{x}={dir}.bbmRM{reference}/{sample}.{x}",x=bbstats,**wc),
             minid=0.95,
             maxindel=3,
             bwr=0.16,
             bw=12,
             trimq=10,
             qtrim="rl",
             flags="quickmatch fast untrim machineout",
             minhits=2,
             mem=icfg.mem("23g"),
             inout2="in2={input[1]} outu2={output.clean[1]} outm2={output.human[1]}"
    threads: 16
    conda:   "bbmap.yml"
    shell:
        "bbmap.sh "
        " minid={params.minid} "
        " maxindel={params.maxindel} "
        " bwr={params.bwr} "
        " bw={params.bw} "
        " {params.flags} "
        " minhits={params.minhits} "
        " path={input.reference} "
        " qtrim={params.qtrim} "
        " trimq={params.trimq} "
        " -Xmx{params.mem}m "
        " in={input[0]} "
        " outu={output.clean[0]} "
        " outm={output.human[0]} "
        " {params.inout2} "
        " threads={threads} "
        " {params.stats} "
        " > {log} 2>&1"

workflow.derive_rule(
    name="bbmap_split_se",
    parent="bbmap_split",
    input="{dir}/{sample}.{:pairnames[0]:}.fq.gz",
    output={
        "clean": ["{dir}.bbmRM{reference}/{sample}.{:pairnames[0]:}.fq.gz"],
        "human": ["{dir}.bbmKP{reference}/{sample}.{:pairnames[0]:}.fq.gz"]
    },
    params={"inout2":""},
    order="lesser"
)


rule bmtagger_bitmask:
    message: "Bmtool indexing {input}"
    input:   "{path}.fasta"
    output:  "{path}.bitmask"
    log:     "{path}.bitmask.log"
    threads: 1
    params:  wordsize = 18,
             mem = icfg.mem("16g")
    conda:   "bmtagger.yml"
    shell: """
    bmtool \
        --fasta-file={input} \
        --output-file={output} \
        --word-size={params.wordsize} \
        > {log} 2>&1
    """
    # --compress fails with small references (segfault in bmfilter)


rule bmtagger_index:
    message: "Srcprism indexing {input}"
    input:   "{path}.fasta.gz"
    output:  touch("{path}.srprism")
    log:     "{path}.srprism.log"
    threads: 1
    params:  basename="{path}.srprism",
             mem = icfg.mem("8g")
    conda:   "bmtagger.yml"
    shell: """
    srprism mkindex \
        --input {input} \
        --output {output} \
        --memory {params.mem} \
        > {log} 2>&1
    """

ruleorder: gunzip > bmtagger_find
rule bmtagger_find:
    message: "Bmtagger find {wildcards.reference} {wildcards.dir}/{wildcards.sample}"
    input:   "{dir}/{sample}.{:pairnames:}.fq",
             bitmask = "{:dir.references:}/{reference}.bitmask",
             srprism = "{:dir.references:}/{reference}.srprism",
             tmpdir = ancient(icfg.dir.tmp)
    output:  temp("{dir}.bmtaggerRM{reference}/{sample}.txt"),
             "{dir}.bmtaggerRM{reference}/{sample}.txt.gz"
    log:     "{dir}.bmtaggerRM{reference}/{sample}.bmtagger.log"
    threads: 1
    params:  mem = icfg.mem("8g")
    conda:   "bmtagger.yml"
    shell: """
    bmtagger.sh \
        -b {input.bitmask} \
        -x {input.srprism} \
        -q 1 \
        -1 {input[0]} \
        -2 {input[1]} \
        -T {input.tmpdir} \
        -o {output[0]} \
        > {log} 2>&1
    gzip {output[0]} -c > {output[0]}.gz
    """


rule bmtagger_filter:
    message: "Bmtagger removing {wildcards.reference} {wildcards.dir}/{wildcards.sample}"
    wildcard_constraints:
        filt = "(RM|KP)"
    input:   "{dir}/{sample}.{pairsuff}.fq",
             idlist = "{dir}.bmtaggerRM{reference}/{sample}.txt"
    output:  "{dir}.bmtagger{filt}{reference}/{sample}.{pairsuff}.fq.gz"
    log:     "{dir}.bmtagger{filt}{reference}/{sample}.{pairsuff}.extract.log"
    threads: 8
    params:  filt = lambda wc: "-remove" if wc.filt == "RM" else "-keep",
             mem = icfg.mem("8g")
    conda:   "bmtagger.yml"
    shell: """
    extract_fullseq \
        {input.idlist} \
        {params.filt} \
        -fastq \
        -mate1 {input[0]} | pigz -p {threads} -9 > {output}  2>{log}
    """

###
### Primer Filtering
###

rule bbduk_primer:
    """
    Splits reads based on primer matching into "primermatch" and "primerfail".
    """
    message: "BBduk: Filtering {wildcards.sample} for primer set {input.primer}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz",
             primer = "primers.fasta"
    output:  match  = "{dir}.primermatch/{sample}.{:pairnames:}.fq.gz",
             fail   = "{dir}.primerfail/{sample}.{:pairnames:}.fq.gz",
             stats  = expand("{{dir}}.primermatch/{{sample}}.{x}", x=bbduk_stats)
    log:     "{dir}.primermatch/{sample}.log"
    threads: 8
    params:
        stats=lambda wc: expand("{x}={dir}.primermatch/{sample}.{x}",x=bbduk_stats,**wc),
        mem=icfg.mem("80g"),
        k=12,
        rl=12,
        inout2="in2={input[1]} outm2={output.match[1]} outu2={output.fail[1]}"
    conda: "bbmap.yml"
    shell:
        'bbduk.sh'
        ' in={input[0]} outm={output.match[0]} outu={output.fail[0]} '
        ' {params.inout2} '
        ' ref={input.primer}'
        ' k={params.k}'               # match using k-mers
        ' restrictleft={params.rl} '  # only match leftmost n bases
        ' maskmiddle=f'               # don't mask middle base in kmer
        ' rcomp=f'                    # don't check reverse complement
        ' copyundefined=t'            # expand wobbles in input
        ' removeifeitherbad=f'        # "bad" is "match", we want both to match
        ' pigz unpigz'
        ' {params.stats}'
        ' -Xmx{params.mem}m'


workflow.derive_rule(
    name="bbduk_primer_se",
    parent="bbduk_primer",
    input="{dir}/{sample}.{:pairnames[0]:}.fq.gz",
    output={
        "match": ["{dir}.primermatch/{sample}.{:pairnames[0]:}.fq.gz"],
        "fail":  ["{dir}.primerfail/{sample}.{:pairnames[0]:}.fq.gz"]
    },
    params={"inout2":""},
    order="lesser"
)

###
### De-duplication
###

        
rule bbmap_dedupe:
    message: "BBTools dedupe'ing {input}"
    input:   "{dir}/{sample}.{:pairnames:}.fq.gz"
    output:  "{dir}.ddp/{sample}.{:pairnames:}.fq.gz"
    log:     "{dir}.ddp/{sample}.log"
    params: mem=icfg.mem("80g")
    threads: 4
    conda:   "bbmap.yml"
    shell:
        "dedupe.sh"
        " unpigz"
        " threads={threads}"
        " in={input[0]}"
        " in2={input[1]}"
        " out=stdout"
        " -Xmx{params.mem}m "
        " 2>{log}"
        " |"
        " paste - - - -  - - - - | "
        " tee >(cut -f 1-4 | tr \"\t\" \"\\n\" | pigz -p {threads} > {output[0]}) | "
        " cut -f 5-8 | tr \"\t\" \"\\n\" | "
        " pigz -p {threads} > {output[1]}"

rule bbmap_dedupe_se:
    message: "BBTools dedupe'ing {input}"
    input:   "{dir}/{sample}.{:pairnames[0]:}.fq.gz"
    output:  "{dir}.ddp/{sample}.{:pairnames[0]:}.fq.gz"
    log:     "{dir}.ddp/{sample}.log"
    params: mem=icfg.mem("80g")
    threads: 4
    conda:   "bbmap.yml"
    shell:
        "dedupe.sh"
        " unpigz"
        " threads={threads}"
        " in={input[0]}"
        " out=stdout"
        " -Xmx{params.mem}m "
        " 2>{log}"
        " |"
        " pigz -p {threads} > {output[0]}"

ruleorder: bbmap_dedupe > bbmap_dedupe_se


