projects:
  # Projects configure the sequencing projects you wish to analyse
  toy:
    # The `data` field specifies a TSV, CSV or Excel sheet containing
    # the experiment matrix. It should have at least one column each
    # for forward and reverse reads (if available) and an identifying
    # (=unique) column. Additional data columns can be used for
    # grouping data (e.g. for co-assemblies within subjects sampled
    # multiple timees, or to merge technical replicates).
    # The field may be a list of files, which will be concatenated.
    data: 'toy_data/toy.txt'
  mock:
    data: 'mock_data/mock.csv'
  mpic:
    # If a barcode column is specified, this column is expected to
    # contain the name of a barcode file for demultiplexing reads.
    barcode_col: 'barcodes'
    data:
      - paste:
          # You can also combine files by adding columns, rather
          # than rows. Paste assumes identical order.
          - 'mpic_data/map.tsv'
          - table:
              # Or create a table directly within this configuration.
              - forward_reads: 'mpic_data/forward_reads.fastq.gz'
              - barcodes: 'mpic_data/barcodes.fastq.gz'

references:
  # References configure static reference data.
  ssu:
    # The URL points to a single, local or remote file.
    - url: 'toy_data/ssu.fasta.gz'
  genome:
    - url: 'toy_data/reference_1K.fa.gz'
  query:
    # If the file type cannot be deduced from the file name,
    # you should specify it explicitly:
    - url: 'toy_data/query.faa'
      type: fastp
  primers:
    - url: 'mpic_data/primers.fasta'

overrides:
  rules:
    # Override any rule parameter for all workflows
    # executed with this configuration.
    bmtagger_bitmask:
      params:
        wordsize: 12
    humann2:
      params:
        chocophlan: 'DEMO'
        uniref: 'DEMO_diamond'
    metabat2_bin:
      params:
        min_cls_size: 2000

resource_limits:
  # This section allows capping and scaling resources (including the
  # special resource threads). For each resource, the mininum and
  # maximum to which rule settings should be bounded can be specified, a
  # default for rules not setting a value, and a scale factor that will
  # be applied to values set in rules. The format parameter allows
  # parsing and formatting k/m/g/t style numbers (number) and time
  # values (time).
  #
  # The resource modifications will be applied after overrides.
  mem:
    # Use "mem" in "resources:" to specify how much RAM a rule will need
    # for execution. Suffixes k, m, g and t will be parsed and the value
    # in MB and GB made available via "{resources.mem_mb}" and
    # "{resources.mem_gb}", respectively.
    default: 20M
    scale: 1
    min: 20M
    max: 4G
  walltime:
    # Use "walltime" to specify maximum execution times for submitting
    # jobs to a cluster. The time is parsed and formatted in SLURM
    # syntax by default.
    default: 59:59
  threads:
    # This allows capping and rescaling job thread counts, which is
    # mostly useful for cluster execution where job thread count is
    # otherwise unbounded. Set the max to the maximum thread count
    # submitted jobs should have.
    max: 128

pipelines:
  # Pipelines are sequential combinations of stages. Once your workflow
  # becomes longer, this will be required to keep filename lengths
  # below 254 characters. It also helps with the amount of typing.
  # Warning: Change pipelines with care. Changes do not (always)
  #          automatically lead to recomputation!
  mytrim:
    # Use as shortcut for a single stage:
    stages:
      - trim_bbmap
  myassemble:
    stages:
      - dust_bbmapE60:
          hide: true
      - assemble_megahit
  mymap:
    # Use as shortcut for a few stages:
    hide: true
    stages:
      - index_bowtie2
      - map_bowtie2
      - sort_bam:
          hide: false
  mypipeline:
    # Use pipelines in pipelines:
    stages:
      - mytrim
      - myassemble
      - mymap
