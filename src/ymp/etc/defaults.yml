# Default directory structure
directories:
    tmp: !workdir 'tmp'
    references: !workdir 'references'
    dbs: !workdir 'dbs'
    scratch: !workdir 'scratch'
    reports: !workdir 'reports'
    sra: !workdir 'sra'
    log: !workdir 'log'
    rules:
      builtin: ../rules
      workdir: !workdir 'rules'
    downloads: !workdir 'downloads'
    dynamic_envs: &dynamic_envs !workdir '.ymp/envs'
    conda_envs: &conda_envs !workdir 'conda_envs'
    conda_prefix: '~/.ymp/conda'
    conda_archive_prefix: "~/.ymp/conda_archive"

conda:
  frontend: mamba
  # If set, use frozen environments from this set
  env_specs:
    - *conda_envs
    #- ../conda_envs/latest

  # Search path for <envname>.yml files:
  env_path:
    10: "."
    20: *dynamic_envs

  # Parameters for generating environments:
  defaults:
    none:
      channels: []
      dependencies: []
    bioconda:
      channels:
        - conda-forge
        - bioconda
      dependencies: []
    conda-forge:
      channels:
        - conda-forge
      dependencies: []
  # Alternate URL patterns
  alturls:
    - /bioconda/bioconda-legacy/ # try bioconda-legacy
    - /conda-forge/conda-forge\/label\/broken/
    - /conda-forge/conda-forge\/label\/cf201901/  # gcc4
    - /conda-forge/conda-forge\/label\/old_feature_broken/  # gcc4
  create: # defaults for env creation
    reinstall: false  # always install again, used by --fresh option
    noarchive: false  # delete archive files before creating
    nospec: false  # no not use spec, always calculate new package set

# Default references
references:
  # Human Genomes
  hs37:
    - url: https://ftp.ncbi.nlm.nih.gov/pub/agarwala/bmtagger/hs37.fa
  hs37d5:
    - url: ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz
  hs37d5ss:
    - url: ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5ss.fa.gz
  hg18:
    - url: http://deweylab.biostat.wisc.edu/rsem/human_refseq_NMonly_125bpPolyATail_extractedFromHumanGenome_hg18.tar.gz
  GRCh38:  # hisat2 default hg reference
    - url: ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/grch38_snp_tran.tar.gz
      strip_components: 1
      type: dir
      files:
        ALL.1.ht2: genome_snp_tran.1.ht2
        ALL.2.ht2: genome_snp_tran.2.ht2
        ALL.3.ht2: genome_snp_tran.3.ht2
        ALL.4.ht2: genome_snp_tran.4.ht2
        ALL.5.ht2: genome_snp_tran.5.ht2
        ALL.6.ht2: genome_snp_tran.6.ht2
        ALL.7.ht2: genome_snp_tran.7.ht2
        ALL.8.ht2: genome_snp_tran.8.ht2
    - url: ftp://ftp.ensembl.org/pub/release-84/gtf/homo_sapiens/Homo_sapiens.GRCh38.84.gtf.gz
      type: gtf
    - url: ftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz
      type: fasta
    - url: http://hgdownload.cse.ucsc.edu/goldenPath/hg38/database/snp144Common.txt
      type: snp
  # Synthetic Sequences
  phiX:
    - url: https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/819/615/GCF_000819615.1_ViralProj14015/GCF_000819615.1_ViralProj14015_genomic.fna.gz
      type: fasta
  UniVec:
    - url: https://ftp.ncbi.nlm.nih.gov/pub/UniVec/UniVec
      type: fasta

  # rRNA Sequences
  silva:
    - url: https://www.arb-silva.de/fileadmin/silva_databases/release_128/Exports/SILVA_128_SSURef_Nr99_tax_silva_trunc.fasta.gz
      type: fasta
  mothur_SEED:
    - url: https://www.mothur.org/w/images/a/a4/Silva.seed_v128.tgz
      type: dir
      files:
        - silva.seed_v128.tax
        - silva.seed_v128.align
  greengenes:
    - url: ftp://greengenes.microbio.me/greengenes_release/gg_13_5/gg_13_8_otus.tar.gz
      type: dir
      strip_components: 1
      files:
        - rep_set/99_otus.fasta
        - rep_set/97_otus.fasta
        - rep_set/94_otus.fasta
  # Tool Specific
  phyloFlash:
    - url: "None"
  metaphlan2:
    - url: https://depot.galaxyproject.org/software/metaphlan2/metaphlan2_2.6.0_src_all.tar.gz
      type: dir
      strip_components: 1
      files:
        - db_v20/mpa_v20_m200.1.bt2
        - db_v20/mpa_v20_m200.2.bt2
        - db_v20/mpa_v20_m200.3.bt2
        - db_v20/mpa_v20_m200.4.bt2
        - db_v20/mpa_v20_m200.rev.1.bt2
        - db_v20/mpa_v20_m200.rev.2.bt2
        - db_v20/mpa_v20_m200.pkl
  centrifuge:
    - url: ftp://ftp.ccb.jhu.edu/pub/infphilo/centrifuge/data/p+h+v.tar.gz
      type: dir
      files:
        - p+h+v.1.cf
        - p+h+v.2.cf
        - p+h+v.3.cf
    - url: ftp://ftp.ccb.jhu.edu/pub/infphilo/centrifuge/data/nt.tar.gz
      type: dir
      files:
        - nt.1.cf
        - nt.2.cf
        - nt.3.cf
    - url: ftp://ftp.ccb.jhu.edu/pub/infphilo/centrifuge/data/p_compressed+h+v.tar.gz
      type: dir
      files:
        - p_compressed+h+v.1.cf
        - p_compressed+h+v.2.cf
        - p_compressed+h+v.3.cf
    - url: ftp://ftp.ccb.jhu.edu/pub/infphilo/centrifuge/data/p_compressed.tar.gz
      type: dir
      files:
        - p_compressed.1.cf
        - p_compressed.2.cf
        - p_compressed.3.cf
  # Proteins
  swissprot:
    - url: ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz
      type: fastp
  trembl:
    - url: ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_trembl.fasta.gz
      type: fastp

# Default projects (none)
projects: {}

# Default overrides (none)
overrides: {}

# Default resource limits
#
# This section allows capping and scaling resources (including the
# special resource threads). For each resource, the mininum and
# maximum to which rule settings should be bounded can be specified, a
# default for rules not setting a value, and a scale factor that will
# be applied to values set in rules. The format parameter allows
# parsing and formatting k/m/g/t style numbers (number) and time
# values (time).
#
# The resource modifications will be applied after overrides.
resource_limits:
  # Use "mem" in "resources:" to specify how much RAM a rule will need
  # for execution. Suffixes k, m, g and t will be parsed and the value
  # in MB and GB made available via "{resources.mem_mb}" and
  # "{resources.mem_gb}", respectively.
  mem:
    format: number
    default: 50M
    scale: 1
    min: 10M
    max: 1T
  # Do not remove mem_mb or mem_gb as these are used by the built-in
  # stages.
  mem_mb:
    format: number
    unit: "m"
    from: "mem"
  mem_gb:
    format: number
    unit: "g"
    from: "mem"
  # Use "walltime" to specify maximum execution times for submitting
  # jobs to a cluster. The time is parsed and formatted in SLURM
  # syntax by default.
  walltime:
    format: time
    default: 23:59:59
    scale: 1
    min: 1:00
    max: 365-0
  # This allows capping and rescaling job thread counts, which is
  # mostly useful for cluster execution where job thread count is
  # otherwise unbounded. Set the max to the maximum thread count
  # submitted jobs should have.
  threads:
    default: 1
    scale: 1
    min: 1
    max: 1024

# Default cluster configs
cluster:
  profile:                     # add this profile (from profiles:)
  profiles:
    default:
      snake_config:            # snakemake cluster-config file name (relative to project dir)
      drmaa: False             # submit via drmaa
      sync: False              # submit in sync mode
      immediate: False         # submit all jobs at once
      wrapper:                 # job wrapper script
      max_jobs_per_second: 100 # max jobs submitted per second
      latency_wait: 60         # wait for NFS files to appear
      # we have the standard wildcards input.x, output.x plus
      # - dependencies
      # - cluster.x (values from snakemake cluster config)
      # - rule (rule name)
      args: {}                 # arguments for job submission
      nodes: 1024              # max jobs queued to cluster engine
      cores: 1024              # max cores
      local_cores: 4           # max threads used on submit host
      scriptname: "ymp.{rulename}.{jobid}.sh"
      command:

    # Dummy profile, running things locally
    dummy:
      command:  "sh"     # command for job submission
      sync_arg: ""            # parameter for sync mode
      nodes: 2
    # Profile for Torque engine
    torque:
      command:    "qsub"
      sync_arg:   "-sync"
      args:
        log:      "-j oe -o {:ensuredir.log:}/"
        name:     "-N ymp.{rule}"
        threads:  "-l nodes=1:ppn={threads}"
        walltime: "-l walltime={resources.walltime}"
        memory:   "-l mem={resources.mem_mb}"
    # Profile for Gridengine
    gridengine:
      command:    "qsub"
      sync_arg:   "-sync y"
      args:
        log:      "-j y -o {:ensuredir.log:}/"
        name:     "-N ymp.{rule}"
    # Profile for SLURM engine
    slurm:
      command:    "sbatch --parsable"
      sync_arg:   "--wait"
      args:
        log:      "--output={:ensuredir.log:}/%x_%J.log"
        threads:  "--cpus-per-task={threads} --nodes=1 --ntasks=1"
        memory:   "--mem={resources.mem_mb}"
        walltime: "--time={resources.walltime}"
      cluster_status: "python -m ymp.cluster slurm status"
      cluster_cancel: "scancel"
    lsf:
      command:    "python -m ymp.cluster lsf submit"
      args:
        log:      "-o {:ensuredir.log:}/%J.log"
        threads:  "-R 'span[hosts=1]' -n {threads}"
        memory:   "-R 'select[mem>{resources.mem_mb}] rusage[mem={resources.mem_mb}]'"
#       walltime: "-W {resources.walltime}"  # [hours:]minutes - fixme check format
      cluster_status: "python -m ymp.cluster lsf status"

# Internal:
pairnames:
    - R1
    - R2

shell: "/bin/bash"
